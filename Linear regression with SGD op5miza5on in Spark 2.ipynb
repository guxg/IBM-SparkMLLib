{
    "nbformat_minor": 1, 
    "cells": [
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import org.apache.spark.mllib.regression.{LabeledPoint,LinearRegressionWithSGD}\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.mllib.linalg.{Vector,Vectors}"
        }, 
        {
            "execution_count": 29, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "credentials = Map(apiKey -> 1OcYKgYhwRYrcTmooNxtQnmT5f2P5AhDBVJmZnQJ5TbX, serviceId -> iam-ServiceId-7d4019a9-639e-45a5-beca-c0c18f60292a, endPoint -> https://s3-api.us-geo.objectstorage.service.networklayer.com, iamServiceEndpoint -> https://iam.ng.bluemix.net/oidc/token)\nconfigurationName = os_30ac23a0f916475aac293915c17c261c_configs\ncos = com.ibm.ibmos2spark.CloudObjectStorage@30d7a96d\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "lastException: Throwable = null\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "execution_count": 29, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "com.ibm.ibmos2spark.CloudObjectStorage@30d7a96d"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "// The code was removed by Watson Studio for sharing."
        }, 
        {
            "execution_count": 30, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+------------------------------------------------------------------------------------------------+\n| 0.00632  18.00   2.310  0  0.5380  6.5750  65.20  4.0900   1  296.0  15.30 396.90   4.98  24.00|\n+------------------------------------------------------------------------------------------------+\n|                                                                             0.02731   0.00  ...|\n|                                                                             0.02729   0.00  ...|\n|                                                                             0.03237   0.00  ...|\n|                                                                             0.06905   0.00  ...|\n|                                                                             0.02985   0.00  ...|\n+------------------------------------------------------------------------------------------------+\nonly showing top 5 rows\n\n"
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "dfData1 = [ 0.00632  18.00   2.310  0  0.5380  6.5750  65.20  4.0900   1  296.0  15.30 396.90   4.98  24.00: string]\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "execution_count": 30, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[ 0.00632  18.00   2.310  0  0.5380  6.5750  65.20  4.0900   1  296.0  15.30 396.90   4.98  24.00: string]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "val dfData1 = spark.\n    read.format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\").\n    option(\"header\", \"true\").\n    option(\"inferSchema\", \"true\").\n    load(cos.url(\"sparkmldatatypes-donotdelete-pr-avzxrdgnnkhrc4\", \"housing.csv\"))\ndfData1.show(5)"
        }, 
        {
            "execution_count": 31, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "root\n |--  0.00632  18.00   2.310  0  0.5380  6.5750  65.20  4.0900   1  296.0  15.30 396.90   4.98  24.00: string (nullable = true)\n\n"
                }
            ], 
            "source": "dfData1.printSchema()"
        }, 
        {
            "execution_count": 22, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": false
            }, 
            "outputs": [
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "regressionDataSet = MapPartitionsRDD[60] at rdd at <console>:52\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "execution_count": 22, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "MapPartitionsRDD[60] at rdd at <console>:52"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "val regressionDataSet = dfData1.map{ r =>\n    val columns = r.getAs[String](0).split(' ').filter(!_.isEmpty)\n    LabeledPoint(columns(13).toDouble, Vectors.dense(\n    columns(0).toDouble,\n    columns(1).toDouble,\n    columns(2).toDouble,\n    columns(3).toDouble,\n    columns(4).toDouble,\n    columns(5).toDouble,\n    columns(6).toDouble,\n    columns(7).toDouble        \n    ))\n}.rdd.cache()\n\n//regressionDataSet.collect().foreach(println(_))"
        }, 
        {
            "execution_count": 27, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "numIterations = 1000\nstepsSGD = 0.001\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "execution_count": 27, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "0.001"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "val numIterations = 1000\nval stepsSGD = .001"
        }, 
        {
            "execution_count": 24, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "myModel = org.apache.spark.mllib.regression.LinearRegressionModel: intercept = 0.0, numFeatures = 8\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "warning: there was one deprecation warning; re-run with -deprecation for details\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "execution_count": 24, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "org.apache.spark.mllib.regression.LinearRegressionModel: intercept = 0.0, numFeatures = 8"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "val myModel = LinearRegressionWithSGD.train(regressionDataSet,numIterations,stepsSGD)"
        }, 
        {
            "execution_count": 28, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 28, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "Name: org.apache.spark.SparkException\nMessage: Task not serializable\nStackTrace:   at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:345)\n  at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:335)\n  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:159)\n  at org.apache.spark.SparkContext.clean(SparkContext.scala:2292)\n  at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:371)\n  at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:370)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.RDD.map(RDD.scala:370)\n  ... 42 elided\nCaused by: java.io.NotSerializableException: com.ibm.ibmos2spark.CloudObjectStorage\nSerialization stack:\n\t- object not serializable (class: com.ibm.ibmos2spark.CloudObjectStorage, value: com.ibm.ibmos2spark.CloudObjectStorage@4db08499)\n\t- field (class: $iw, name: cos, type: class com.ibm.ibmos2spark.CloudObjectStorage)\n\t- object (class $iw, $iw@3a8cbab4)\n\t- field (class: $iw, name: $iw, type: class $iw)\n\t- object (class $iw, $iw@3873d3a5)\n\t- field (class: $iw, name: $iw, type: class $iw)\n\t- object (class $iw, $iw@18656591)\n\t- field (class: $iw, name: $iw, type: class $iw)\n\t- object (class $iw, $iw@f5a69c9)\n\t- field (class: $iw, name: $iw, type: class $iw)\n\t- object (class $iw, $iw@6df8d2b7)\n\t- field (class: $iw, name: $iw, type: class $iw)\n\t- object (class $iw, $iw@5683c0c8)\n\t- field (class: $line42.$read, name: $iw, type: class $iw)\n\t- object (class $line42.$read, $line42.$read@5da37705)\n\t- field (class: $iw, name: $line42$read, type: class $line42.$read)\n\t- object (class $iw, $iw@6e2f6f03)\n\t- field (class: $iw, name: $outer, type: class $iw)\n\t- object (class $iw, $iw@3ec6d513)\n\t- field (class: $anonfun$1, name: $outer, type: class $iw)\n\t- object (class $anonfun$1, <function1>)\n  at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)\n  at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n  at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\n  at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:342)"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "val predictedLabelValue = regressionDataSet.map { lp =>\n    val predictedValue = myModel.predict(lp.features)\n    (lp.label, predictedValue)\n}\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "println(\"Intercept set:\",myModel.intercept)\nprintln(\"Model Weights:\",myModel.weights)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "predictedLabelValue.takeSample(false,5).foreach(println(_))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "val MSE = predictedLabelValue.map{ case(l, p) => math.pow((l - p),\n   2)}.reduce(_ + _) / predictedLabelValue.count"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "val RMSE = math.sqrt(MSE)\nprintln(\"training Mean Squared Error = \" + MSE)\nprintln(\"training Root Mean Squared Error = \" + RMSE)\n"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Scala 2.11 with Spark", 
            "name": "scala", 
            "language": "scala"
        }, 
        "language_info": {
            "mimetype": "text/x-scala", 
            "version": "2.11.8", 
            "name": "scala", 
            "pygments_lexer": "scala", 
            "file_extension": ".scala", 
            "codemirror_mode": "text/x-scala"
        }
    }, 
    "nbformat": 4
}